% Key References for GES Paper

@article{strubell2019energy,
  title={Energy and Policy Considerations for Deep Learning in {NLP}},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  journal={arXiv preprint arXiv:1906.02243},
  year={2019}
}

@article{schwartz2020green,
  title={Green {AI}},
  author={Schwartz, Roy and Dodge, Jesse and Smith, Noah A and Etzioni, Oren},
  journal={Communications of the ACM},
  volume={63},
  number={12},
  pages={54--63},
  year={2020},
  publisher={ACM}
}

@article{patterson2021carbon,
  title={Carbon Emissions and Large Neural Network Training},
  author={Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  journal={arXiv preprint arXiv:2104.10350},
  year={2021}
}

@article{luccioni2023bloom,
  title={Estimating the Carbon Footprint of {BLOOM}, a 176{B} Parameter Language Model},
  author={Luccioni, Alexandra Sasha and Viguier, Sylvain and Ligozat, Anne-Laure},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={253},
  pages={1--15},
  year={2023}
}

@software{codecarbon,
  title={CodeCarbon: Track and Reduce {CO2} Emissions from your Computing},
  author={{CodeCarbon Contributors}},
  url={https://github.com/mlco2/codecarbon},
  year={2020}
}

@inproceedings{carbontracker,
  title={Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models},
  author={Anthony, Lasse F. Wolff and Kanding, Benjamin and Selvan, Raghavendra},
  booktitle={ICML Workshop on Challenges in Deploying and Monitoring Machine Learning Systems},
  year={2020}
}

@misc{sci,
  title={{SCI} for {AI}: Software Carbon Intensity for Artificial Intelligence},
  author={{Green Software Foundation}},
  url={https://sci-for-ai.greensoftware.foundation/},
  year={2024}
}

@article{mlperf,
  title={{MLPerf} Power: Benchmarking the Energy Efficiency of Machine Learning Systems from Microwatts to Megawatts for Sustainable {AI}},
  author={Reddi, Vijay Janapa and others},
  journal={arXiv preprint arXiv:2410.12032},
  year={2024}
}

@inproceedings{han2015deep,
  title={Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  booktitle={International Conference on Learning Representations},
  year={2016}
}

@inproceedings{sanh2019distilbert,
  title={{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  booktitle={NeurIPS EMC2 Workshop},
  year={2019}
}

@article{howard2017mobilenets,
  title={{MobileNets}: Efficient Convolutional Neural Networks for Mobile Vision Applications},
  author={Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  journal={arXiv preprint arXiv:1704.04861},
  year={2017}
}

@article{tan2019efficientnet,
  title={{EfficientNet}: Rethinking Model Scaling for Convolutional Neural Networks},
  author={Tan, Mingxing and Le, Quoc},
  journal={International Conference on Machine Learning},
  pages={6105--6114},
  year={2019}
}

@article{hu2021lora,
  title={{LoRA}: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}
